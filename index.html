<!DOCTYPE html>
<html>
<head>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <title>
        Reinforcement Learning for Autonomous Unmanned Aerial Vehicles -
        Undergraduate Thesis
    </title>
    <link href="style.css" rel="stylesheet">
</head>

<body>
<div class="sidenav">
    <a href="#home">Home</a>
    <a href="#description">Description</a>
    <a href="#gettingStarted">Getting Started</a>
    <a href="#usage">Usage</a>
    <a href="#about">About</a>
</div>
<div class="main">
    <h1 id="home">
        Reinforcement Learning for Autonomous Unmanned Aerial Vehicles -
        Undergraduate Thesis
    </h1>
    <a href="https://github.com/NickGeramanis/undergrad-thesis-rl-uav">
        <img src="images/github.png" alt="GitHub image"
             style="width: 15%; height: 15%"/>
    </a>
    <h2>
        Implementation of different reinforcement learning algorithms to solve
        the navigation problem of an unmanned aerial vehicle (UAV) using
        <a href="https://www.ros.org/">ROS</a>
        /
        <a href="http://gazebosim.org/">Gazebo</a>
        and Python.
    </h2>
    <h3 id="description">Description</h3>
    <p>
        <quote>navigation_env.py</quote>
        : The goal of this environment is to navigate an UAV on a
        track without crashing into the walls. Initially, the UAV is placed randomly
        into the track but at a safe distance from the walls. The state space consists
        of 5 range measurements. The action space consists of 3 actions (move_forward,
        rotate_left, rotate_right). Furthermore, both actions and states have additive
        white Gaussian noise. The UAV is rewarded +5 for moving forward and -0.5 for
        rotating. If the UAV crashes into the wall it is penalized with -200.
    </p>
    <p>There are 3 available worlds/tracks:</p>
    <p>Track1:</p>
    <img src="images/track1.png" alt="Track1"/>
    <p>Track2:</p>
    <img src="images/track2.png" alt="Track2"/>
    <p>Track3:</p>
    <img src="images/track3.png" alt="Track3"/>
    <h4>Abstract</h4>
    <p>
        Reinforcement learning is an area of machine learning concerned with
        how autonomous agents learn to behave in unknown environments through
        trial-and-error. The goal of a reinforcement learning agent is to learn
        a sequential decision policy that maximizes the notion of cumulative
        reward through continuous interaction with the unknown environment. A
        challenging problem in robotics is the autonomous navigation of an
        Unmanned Aerial Vehicle (UAV) in worlds with no available map. This
        ability is critical in many applications, such as search and rescue
        operations or the mapping of geographical areas. In this thesis, we
        present a map-less approach for the autonomous, safe navigation of a
        UAV in unknown environments using reinforcement learning. Specifically,
        we implemented two popular algorithms, SARSA(λ) and Least-Squares
        Policy Iteration (LSPI), and combined them with tile coding, a
        parametric, linear approximation architecture for value function in
        order to deal with the 5- or 3-dimensional continuous state space
        defined by the measurements of the UAV distance sensors. The final
        policy of each algorithm, learned over only 500 episodes, was tested in
        unknown environments more complex than the one used for training in
        order to evaluate the behavior of each policy. Results show that
        SARSA(λ) was able to learn a near-optimal policy that performed
        adequately even in unknown situations, leading the UAV along paths
        free-of-collisions with obstacles. LSPI's policy required less learning
        time and its performance was promising, but not as effective, as it
        occasionally leads to collisions in unknown situations. The whole
        project was implemented using the Robot Operating System (ROS)
        framework and the Gazebo robot simulation environment.
    </p>
    <p>Supervisor: Associate Professor Michail G. Lagoudakis</p>
    <a href=https://doi.org/10.26233/heallink.tuc.87066>https://doi.org/10.26233/heallink.tuc.87066</a>
    <h3 id="gettingStarted">Getting Started</h3>
    <h4>Prerequisites</h4>
    <p>
        This package is provided as a docker image.
    </p>
    <h4>Installation</h4>
    <p>Build the docker image:</p>
    <code>docker build -t rl-uav .</code>
    <p>Run the container:</p>
    <code>docker run -it rl-uav</code>
    <p>Due to XmlRpcServer querying all possible file descriptors, it may be required to lower the corresponding limit depending on your system:</p>
    <code>docker run --ulimit nofile=1024:524288 -it rl-uav</code>
    <p>Build the ros package:</p>
    <code>
        cd /home/catkin_ws<br>
        catkin_make
    </code>
    <h3 id="usage">Usage</h3>
    <p>In order to launch a new world you must start the
        <quote>train.launch</quote>
        file:
    </p>
    <code>
        source /home/catkin_ws/devel/setup.bash<br>
        roslaunch rl_uav train.launch world:=track1 gui:=false
    </code>
    <p>
        After the world has started, run the
        <quote>train_uav</quote>
        node (
        <quote>train_uav.py</quote>
        ) to begin the training process and test different algorithms:
    </p>
    <code>
        source /home/catkin_ws/devel/setup.bash<br>
        rosrun rl_uav train_uav.py
    </code>
    <h3 id="about">About</h3>
    <h4>Status</h4>
    <p>Under maintenance.</p>
    <h4>License</h4>
    <p>
        Distributed under the GPL-3.0 License. See
        <quote>LICENSE</quote>
        for more information.
    </p>
    <h4>Authors</h4>
    <a href="https://www.linkedin.com/in/nikolaos-geramanis">Nick Geramanis</a>
</div>
</body>
</html>
